{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import k3d\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# ---------- KITTI Utilities ---------- #\n",
    "\n",
    "# Load LIDAR point cloud from a .bin file\n",
    "def load_lidar_bin(file_path):\n",
    "    # Load binary data and reshape to N x 4 (x, y, z, reflectance)\n",
    "    points = np.fromfile(file_path, dtype=np.float32).reshape(-1, 4)\n",
    "    # Return only x, y, z (ignore reflectance)\n",
    "    return points[:, :3]\n",
    "\n",
    "# Extract object detection labels from KITTI label file\n",
    "def extract_labels(file_path):\n",
    "    columns = [\n",
    "        \"ObjectType\", \"Truncation\", \"Occlusion\", \"Alpha\", \n",
    "        \"X1\", \"Y1\", \"X2\", \"Y2\", \"H\", \"W\", \"L\", \"X\", \"Y\", \"Z\", \"Rotation_Y\"\n",
    "    ]\n",
    "    data = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            values = line.strip().split()\n",
    "            # Create a dictionary for each object, converting all values to float except ObjectType\n",
    "            object_data = {\n",
    "                columns[i]: values[i] if i == 0 else float(values[i])\n",
    "                for i in range(len(columns))\n",
    "            }\n",
    "            data.append(object_data)\n",
    "    # Return as a Pandas DataFrame\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Extract object detection labels from KITTI label file\n",
    "def extract_predictions(file_path):\n",
    "    columns = [\n",
    "        \"ObjectType\", \"Truncation\", \"Occlusion\", \"Alpha\", \n",
    "        \"X1\", \"Y1\", \"X2\", \"Y2\", \"L\", \"H\", \"W\", \"X\", \"Y\", \"Z\", \"Rotation_Y\"\n",
    "    ]\n",
    "    data = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            values = line.strip().split()\n",
    "            # Create a dictionary for each object, converting all values to float except ObjectType\n",
    "            object_data = {\n",
    "                columns[i]: values[i] if i == 0 else float(values[i])\n",
    "                for i in range(len(columns))\n",
    "            }\n",
    "            data.append(object_data)\n",
    "    # Return as a Pandas DataFrame\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load calibration matrices (projection, rectification, and transformation) from file\n",
    "def extract_matrices(filename):\n",
    "    global P2, P3, R0_rect, Tr_velo_to_cam, Tr_cam_to_velo\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.strip():\n",
    "                parts = line.split(':')\n",
    "                key = parts[0].strip()\n",
    "                values = np.fromstring(parts[1], sep=' ')\n",
    "                # Parse and reshape the appropriate matrix\n",
    "                if key == \"P2\":\n",
    "                    P2 = values.reshape(3, 4)\n",
    "                elif key == \"P3\":\n",
    "                    P3 = values.reshape(3, 4)\n",
    "                elif key == \"R0_rect\":\n",
    "                    R0_rect = values.reshape(3, 3)\n",
    "                elif key == \"Tr_velo_to_cam\":\n",
    "                    matrix_3x4 = values.reshape(3, 4)\n",
    "                    # Extend to 4x4 for homogeneous transformation\n",
    "                    Tr_velo_to_cam = np.vstack([matrix_3x4, np.array([[0, 0, 0, 1]])])\n",
    "                    # Inverse to get camera to lidar transformation\n",
    "                    Tr_cam_to_velo = np.linalg.inv(Tr_velo_to_cam)\n",
    "\n",
    "def compute_3D_from_disparity(right_proj_mat, disparity_img, R0_rect=None):\n",
    "    \"\"\"\n",
    "    Compute a 3D point cloud from a disparity image using the stereo projection matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - right_proj_mat: 3x4 right projection matrix \n",
    "    - disparity_img: 2D array of disparity values\n",
    "    - R0_rect (optional): 3x3 rectification matrix to convert to rectified camera coordinates\n",
    "\n",
    "    Returns:\n",
    "    - xyz_img: (H, W, 3) array of 3D points in camera (or rectified) coordinates\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert disparity from uint16 to float if needed (KITTI format stores disparity * 256)\n",
    "    if disparity_img.dtype == np.uint16:\n",
    "        disparity_img = disparity_img.astype(np.float32) / 256.0\n",
    "\n",
    "    height, width = disparity_img.shape\n",
    "    xyz_img = np.full((height, width, 3), np.nan, dtype=np.float32)  # Default to NaNs\n",
    "\n",
    "    # Extract camera intrinsics\n",
    "    f = right_proj_mat[0, 0]             # Focal length (assumed fx = fy)\n",
    "    c_x = right_proj_mat[0, 2]           # Principal point x\n",
    "    c_y = right_proj_mat[1, 2]           # Principal point y\n",
    "    T_x = right_proj_mat[0, 3]           # Translation offset (fx * baseline)\n",
    "\n",
    "    # Compute baseline from T_x and focal length\n",
    "    baseline = -T_x / f            # baseline (in meters)\n",
    "\n",
    "    # Create a mask for valid disparity values\n",
    "    mask = disparity_img > 0\n",
    "    disparity = disparity_img.copy()\n",
    "\n",
    "    # Compute depth: Z = f * B / disparity\n",
    "    depth = np.full_like(disparity, np.nan, dtype=np.float32)\n",
    "    depth[mask] = f * baseline / disparity[mask]\n",
    "\n",
    "    # Generate mesh grid of pixel coordinates (u, v)\n",
    "    u, v = np.meshgrid(np.arange(width), np.arange(height))\n",
    "\n",
    "    # Compute X, Y, Z in camera coordinates\n",
    "    X = (u - c_x) * depth / f\n",
    "    Y = (v - c_y) * depth / f\n",
    "    Z = depth\n",
    "\n",
    "    # Stack into 3D points\n",
    "    xyz_cam = np.stack((X, Y, Z), axis=-1)\n",
    "\n",
    "    # Apply rectification if needed\n",
    "    if R0_rect is not None:\n",
    "        xyz_flat = xyz_cam.reshape(-1, 3)\n",
    "        xyz_rect = R0_rect @ xyz_flat.T\n",
    "        xyz_img = xyz_rect.reshape(height, width, 3)\n",
    "    else:\n",
    "        xyz_img = xyz_cam\n",
    "\n",
    "    # # Optional: mask out distant points (e.g., beyond 17.5 meters)\n",
    "    # xyz_img[xyz_img[:, :, 2] > 17.5] = np.nan\n",
    "    print(xyz_img.shape)\n",
    "\n",
    "    return xyz_img\n",
    "\n",
    "\n",
    "def filter_lidar_fov(points, fov_degrees=120):\n",
    "    \"\"\"\n",
    "    Filters 3D LIDAR points to keep only those within a given horizontal field of view (in front of the camera).\n",
    "    \n",
    "    Parameters:\n",
    "    - points: Nx3 array of points in camera coordinates\n",
    "    - fov_degrees: field of view angle in degrees (default: 120)\n",
    "\n",
    "    Returns:\n",
    "    - Filtered points within the field of view\n",
    "    \"\"\"\n",
    "    # Compute horizontal angles (in degrees) relative to camera's forward axis\n",
    "    x, y = points[:, 0], points[:, 1]\n",
    "    angles = np.degrees(np.arctan2(y, x))  # Angle in degrees between -180 and 180\n",
    "\n",
    "    # Define limits (centered around 0)\n",
    "    half_fov = fov_degrees / 2.0\n",
    "    mask = (angles >= -half_fov) & (angles <= half_fov)\n",
    "\n",
    "    return points[mask]\n",
    "\n",
    "def remove_ground_plane(points, height_threshold=-1.0, axis='z', direction='above'):\n",
    "    \"\"\"\n",
    "    Removes ground points by filtering points below a certain height.\n",
    "\n",
    "    Parameters:\n",
    "    - points: Nx3 numpy array\n",
    "    - height_threshold: height cutoff\n",
    "    - axis: axis to use for height\n",
    "\n",
    "    Returns:\n",
    "    - filtered_points: points above the ground\n",
    "    \"\"\"\n",
    "    axis_map = {'x': 0, 'y': 1, 'z': 2}\n",
    "    axis_idx = axis_map.get(axis.lower(), 2)\n",
    "\n",
    "    if direction == 'above':\n",
    "        return points[points[:, axis_idx] > height_threshold]\n",
    "    elif direction == 'below':\n",
    "        return points[points[:, axis_idx] < height_threshold]\n",
    "    else:\n",
    "        raise ValueError(\"Direction must be 'above' or 'below'\")\n",
    "\n",
    "def clip_point_cloud(pcd, max_range=20.0):\n",
    "    points = np.asarray(pcd.points)\n",
    "    mask = np.linalg.norm(points, axis=1) < max_range\n",
    "    pcd.points = o3d.utility.Vector3dVector(points[mask])\n",
    "    return pcd\n",
    "\n",
    "def compute_hausdorff_and_stats(pcd1, pcd2):\n",
    "    \"\"\"Compute symmetric Hausdorff and average distances.\"\"\"\n",
    "    d1 = np.asarray(pcd1.compute_point_cloud_distance(pcd2))\n",
    "    d2 = np.asarray(pcd2.compute_point_cloud_distance(pcd1))\n",
    "\n",
    "    hausdorff = max(np.max(d1), np.max(d2))\n",
    "    mean1, mean2 = np.mean(d1), np.mean(d2)\n",
    "    percentile_95 = np.percentile(np.concatenate([d1, d2]), 95)\n",
    "\n",
    "    return hausdorff, mean1, mean2, percentile_95\n",
    "\n",
    "# Transform 3D LIDAR points from LIDAR to camera coordinate system\n",
    "def transform_lidar_to_camera(lidar_points, Tr_velo_to_cam):\n",
    "\n",
    "    # Convert to homogeneous coordinates by adding 1\n",
    "    ones = np.ones((lidar_points.shape[0], 1))\n",
    "    lidar_hom = np.hstack((lidar_points, ones))\n",
    "\n",
    "    # Apply transformation matrix\n",
    "    cam_points = lidar_hom @ Tr_velo_to_cam.T\n",
    "\n",
    "    return cam_points[:, :3]  # Return only x, y, z (discard homogeneous coordinate)\n",
    "\n",
    "# Display stereo image pair side by side\n",
    "def plot_img(left_path, right_path, figsize=(22, 6)):\n",
    "    img_left = mpimg.imread(left_path)\n",
    "    img_right = mpimg.imread(right_path)\n",
    "    combined_img = np.hstack((img_left, img_right))\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(combined_img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def rgb_to_int(rgb_colors):\n",
    "    \"\"\"\n",
    "    rgb_colors: np.array of size Nx3\n",
    "    return: np.array of size N\n",
    "    \"\"\"\n",
    "    # Avoid overflow issues with uint8.\n",
    "    rgb_colors = rgb_colors.astype(np.uint32).T\n",
    "    int_colors = (rgb_colors[0] << 16) + (rgb_colors[1] << 8) + rgb_colors[2]\n",
    "    return int_colors\n",
    "\n",
    "def bgr_to_int(bgr_colors):\n",
    "    \"\"\"\n",
    "    bgr_colors: np.array of size Nx3\n",
    "    return: np.array of size N\n",
    "    \"\"\"\n",
    "    return rgb_to_int(bgr_colors[:, ::-1])\n",
    "\n",
    "# def compute_box_corners(label_row):\n",
    "#     # Dimensions\n",
    "#     h, w, l = label_row['H'], label_row['W'], label_row['L']\n",
    "#     x, y, z = label_row['X'], label_row['Y'], label_row['Z']\n",
    "#     ry = label_row['Rotation_Y']\n",
    "    \n",
    "#     # Create a bounding box in object coordinates\n",
    "#     corners = np.array([\n",
    "#         [ l/2,  l/2, -l/2, -l/2,  l/2,  l/2, -l/2, -l/2],\n",
    "#         [  0 ,   0 ,   0 ,   0 , -h , -h ,  -h ,  -h ],\n",
    "#         [ w/2, -w/2, -w/2,  w/2,  w/2, -w/2, -w/2,  w/2]\n",
    "#     ])\n",
    "\n",
    "#     # Rotation around Y-axis\n",
    "#     R = np.array([\n",
    "#         [ np.cos(ry), 0, np.sin(ry)],\n",
    "#         [ 0,          1, 0         ],\n",
    "#         [-np.sin(ry), 0, np.cos(ry)]\n",
    "#     ])\n",
    "#     rotated = R @ corners\n",
    "#     translated = rotated + np.array([[x], [y], [z]])\n",
    "#     return translated.T  # (8, 3)\n",
    "\n",
    "def filter_labels_in_fov(labels_df, R0_rect, fov_degrees=80):\n",
    "    \"\"\"\n",
    "    Filters KITTI label objects so that at least one bounding box corner lies in the camera's horizontal FOV.\n",
    "\n",
    "    Parameters:\n",
    "        labels_df: KITTI labels dataframe\n",
    "        R0_rect: 3x3 rectification matrix\n",
    "        fov_degrees: horizontal FOV angle in degrees\n",
    "\n",
    "    Returns:\n",
    "        Filtered label dataframe\n",
    "    \"\"\"\n",
    "    def compute_box_corners_camera_coords(row):\n",
    "        h, w, l = row['H'], row['W'], row['L']\n",
    "        x, y, z = row['X'], row['Y'], row['Z']\n",
    "        ry = row['Rotation_Y']\n",
    "\n",
    "        # 3D bounding box in local object frame (camera coordinates)\n",
    "        x_corners = [ l/2,  l/2, -l/2, -l/2,  l/2,  l/2, -l/2, -l/2]\n",
    "        y_corners = [  0 ,   0 ,   0 ,   0 , -h , -h ,  -h ,  -h ]\n",
    "        z_corners = [ w/2, -w/2, -w/2,  w/2,  w/2, -w/2, -w/2,  w/2]\n",
    "\n",
    "        corners = np.array([x_corners, y_corners, z_corners])\n",
    "        \n",
    "        # Rotation around Y-axis\n",
    "        R = np.array([\n",
    "            [ np.cos(ry), 0, np.sin(ry)],\n",
    "            [ 0,          1, 0         ],\n",
    "            [-np.sin(ry), 0, np.cos(ry)]\n",
    "        ])\n",
    "        corners_rot = R @ corners\n",
    "        corners_3d = corners_rot + np.array([[x], [y], [z]])  # Translate\n",
    "        return corners_3d.T  # shape (8, 3)\n",
    "\n",
    "    def in_fov(points_cam, fov_degrees):\n",
    "        x, z = points_cam[:, 0], points_cam[:, 2]\n",
    "        angles = np.degrees(np.arctan2(x, z))  # Horizontal angle\n",
    "        half_fov = fov_degrees / 2.0\n",
    "        return np.any((angles >= -half_fov) & (angles <= half_fov) & (z > 0))\n",
    "\n",
    "    filtered_rows = []\n",
    "    for _, row in labels_df.iterrows():\n",
    "        corners = compute_box_corners_camera_coords(row)\n",
    "        corners_rect = (R0_rect @ corners.T).T\n",
    "        if in_fov(corners_rect, fov_degrees):\n",
    "            filtered_rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(filtered_rows)\n",
    "\n",
    "\n",
    "\n",
    "def draw_3d_bounding_boxes_k3d_with_labels(plot, labels_df, R0_rect, color=0xff0000):\n",
    "    def compute_box_corners_camera_coords(row):\n",
    "        h, w, l = row['H'], row['W'], row['L']\n",
    "        x, y, z = row['X'], row['Y'], row['Z']\n",
    "        ry = row['Rotation_Y']\n",
    "\n",
    "        x_corners = [ l/2,  l/2, -l/2, -l/2,  l/2,  l/2, -l/2, -l/2]\n",
    "        y_corners = [  0 ,   0 ,   0 ,   0 , -h , -h ,  -h ,  -h ]\n",
    "        z_corners = [ w/2, -w/2, -w/2,  w/2,  w/2, -w/2, -w/2,  w/2]\n",
    "\n",
    "        corners = np.array([x_corners, y_corners, z_corners])\n",
    "        \n",
    "        R = np.array([\n",
    "            [ np.cos(ry), 0, np.sin(ry)],\n",
    "            [ 0,          1, 0         ],\n",
    "            [-np.sin(ry), 0, np.cos(ry)]\n",
    "        ])\n",
    "        corners_rot = R @ corners\n",
    "        corners_3d = corners_rot + np.array([[x], [y], [z]])\n",
    "        return corners_3d.T\n",
    "\n",
    "    def get_box_lines(corners):\n",
    "        lines = [\n",
    "            (0,1), (1,2), (2,3), (3,0),\n",
    "            (4,5), (5,6), (6,7), (7,4),\n",
    "            (0,4), (1,5), (2,6), (3,7)\n",
    "        ]\n",
    "        return [(corners[i], corners[j]) for i, j in lines]\n",
    "\n",
    "    for _, row in labels_df.iterrows():\n",
    "        corners = compute_box_corners_camera_coords(row)\n",
    "        corners_rect = (R0_rect @ corners.T).T\n",
    "        lines = get_box_lines(corners_rect)\n",
    "\n",
    "        vertices = []\n",
    "        indices = []\n",
    "\n",
    "        for i, (start, end) in enumerate(lines):\n",
    "            vertices.extend([start, end])\n",
    "            indices.append([2*i, 2*i + 1])\n",
    "\n",
    "        # Draw 3D box\n",
    "        plot += k3d.line(\n",
    "            np.array(vertices, dtype=np.float32),\n",
    "            indices=np.array(indices),\n",
    "            color=color,\n",
    "            width=0.005\n",
    "        )\n",
    "\n",
    "        # Place label above the top face\n",
    "        top_face_indices = [4, 5, 6, 7]  # top face corners\n",
    "        top_center = np.mean(corners_rect[top_face_indices], axis=0)\n",
    "        label_pos = top_center + np.array([0, 0.5, 0])  # Shift text above the box\n",
    "\n",
    "        plot += k3d.text(\n",
    "            row['ObjectType'],\n",
    "            position=label_pos.astype(np.float32),\n",
    "            color=color,\n",
    "            size=1.0,                # Larger text\n",
    "            label_box=False,\n",
    "            is_html=False\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path_dataset = os.path.expanduser(\"~/CV_Simina\")\n",
    "# lidar_relative_path = \"/velodyne/{:06d}.bin\"\n",
    "# left_img_relative_path = \"/image_2/{:06d}.png\"\n",
    "# right_img_relative_path = \"/image_3/{:06d}.png\"\n",
    "# disparity_relative_path = \"/disparity_images/disparity{:06d}.png\"\n",
    "# labels_relative_path = \"/data_object_label_2/{:06d}.txt\"\n",
    "# calib_relative_path =\"/data_object_calib/training/calib/{:06d}.txt\"\n",
    "\n",
    "file_path_dataset = os.path.expanduser(\"~/Computer_vision/kitti_lidar/testing\")\n",
    "file_path_dataset_test = os.path.expanduser(\"~/Computer_vision/Performance/Test_set\")\n",
    "lidar_relative_path = \"/velodyne/{:06d}.bin\"\n",
    "left_img_relative_path = \"/image_2/{:06d}.png\"\n",
    "right_img_relative_path = \"/image_3/{:06d}.png\"\n",
    "disparity_relative_path = \"/disparity_images/disparity{:06d}.png\"\n",
    "labels_relative_path = \"/label_2/{:06d}.txt\"\n",
    "calib_relative_path =\"//calib/{:06d}.txt\"\n",
    "predictions_lidar = \"/lidar/submit/{:06d}.txt\"\n",
    "\n",
    "sequence_number = 924\n",
    "paths = {\"lidar\": file_path_dataset + lidar_relative_path.format(sequence_number),\n",
    "        \"left_img\": file_path_dataset + left_img_relative_path.format(sequence_number),\n",
    "        \"right_img\": file_path_dataset + right_img_relative_path.format(sequence_number),\n",
    "        \"disparity\": file_path_dataset + disparity_relative_path.format(sequence_number),\n",
    "        \"labels\": file_path_dataset + labels_relative_path.format(sequence_number),\n",
    "        \"calib\": file_path_dataset + calib_relative_path.format(sequence_number),\n",
    "        \"pred_lidar\": file_path_dataset_test + predictions_lidar.format(sequence_number)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame: 924\n",
      "   ObjectType  Truncation  Occlusion     Alpha           X1          Y1  \\\n",
      "0  Pedestrian         0.0        0.0  1.028235   418.259003  174.063751   \n",
      "1  Pedestrian         0.0        0.0 -1.068551   803.044800  183.419037   \n",
      "2  Pedestrian         0.0        0.0  1.962524  1082.158813  178.536591   \n",
      "3  Pedestrian         0.0        0.0 -1.234682  1134.553467  195.273392   \n",
      "4     Cyclist         0.0        0.0 -1.524096   686.755310  162.804474   \n",
      "5     Cyclist         0.0        0.0 -1.997118   976.407104  125.850807   \n",
      "6     Cyclist         0.0        0.0 -1.862206   786.330566  175.904388   \n",
      "7         Car         0.0        0.0 -0.985404    25.055443   88.458984   \n",
      "8         Car         0.0        0.0  1.463253   765.929993  176.641907   \n",
      "9         Car         0.0        0.0 -0.659768    11.504411  192.223877   \n",
      "\n",
      "            X2          Y2         L         H         W          X         Y  \\\n",
      "0   473.787201  262.799316  0.847244  1.732670  0.774069  -3.387983  1.758599   \n",
      "1   851.096313  268.764862  0.681058  1.799266  0.780316   4.718853  2.039367   \n",
      "2  1175.274780  324.221832  0.698503  1.661568  0.705365   6.229628  1.734663   \n",
      "3  1242.000000  368.648590  0.743401  1.594718  0.792404   5.883776  1.838680   \n",
      "4   714.439819  235.562744  1.907413  1.829319  0.644198   2.371533  1.577008   \n",
      "5  1037.770142  203.754349  1.770271  1.718387  0.565265   9.168092  0.681954   \n",
      "6   827.429688  241.561249  1.810617  1.681809  0.567686   5.241896  1.768382   \n",
      "7   413.570068  329.553406  5.314191  2.470026  2.072999  -4.887691  1.605752   \n",
      "8   845.603882  242.907837  3.653930  1.543965  1.609048   4.989115  1.653964   \n",
      "9   211.037766  268.449188  3.621329  1.457572  1.541946 -11.424066  1.960293   \n",
      "\n",
      "           Z  Rotation_Y  \n",
      "0  14.675359    0.801348  \n",
      "1  15.853603   -0.779250  \n",
      "2   8.748672    2.581302  \n",
      "3   7.304564   -0.556601  \n",
      "4  19.132915   -1.400774  \n",
      "5  16.816198   -1.497972  \n",
      "6  19.477406   -1.599308  \n",
      "7  10.163290   -1.433668  \n",
      "8  18.955574    1.720617  \n",
      "9  16.750532   -1.258319  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simina/miniconda3/envs/cv_env/lib/python3.10/site-packages/traittypes/traittypes.py:97: UserWarning: Given trait value dtype \"float64\" does not match required type \"float32\". A coerced copy has been created.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a6935168014bb99401071e59111a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Frame: {sequence_number}\")\n",
    "\n",
    "# Load calibration matrices and transform LIDAR points to camera coordinates\n",
    "extract_matrices(paths[\"calib\"])\n",
    "lidar_points = load_lidar_bin(paths[\"lidar\"])\n",
    "lidar_points_cam = filter_lidar_fov(lidar_points, fov_degrees=80)\n",
    "lidar_points_cam = remove_ground_plane(lidar_points_cam, height_threshold=-1.5, axis='z')\n",
    "lidar_points_cam = transform_lidar_to_camera(lidar_points_cam, Tr_velo_to_cam)\n",
    "\n",
    "# ---------- K3D Visualization ---------- #\n",
    "plot = k3d.plot(camera_auto_fit=False, axes_helper=0.0)\n",
    "\n",
    "# Plot LIDAR points in blue\n",
    "lidar_points_cam_rect = (R0_rect @ lidar_points_cam.T).T\n",
    "plot += k3d.points(lidar_points_cam_rect, point_size=0.05, color=0x0000ff)      # Plot rectified LiDAR Pointcloud in green\n",
    "#     plot += k3d.points(lidar_points_cam, point_size=0.05, color=0x00ff00)           # Plot non-rectified Lidar Pointcloud in blue\n",
    "\n",
    "\n",
    "# Load left image and disparity\n",
    "# left_img = cv2.imread(paths[\"left_img\"], cv2.IMREAD_COLOR)\n",
    "# disparity = cv2.imread(paths[\"disparity\"], cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "\n",
    "# xyz_img = compute_3D_from_disparity(P3, disparity)                             # Not rectified, add R0_rect to rectify it\n",
    "\n",
    "## Downsample disparity maps --> only get the forth pixel for example\n",
    "# Subsample disparity map before computing 3D\n",
    "# stride = 4  # every 4th pixel → 16x fewer points\n",
    "\n",
    "# Downsample disparity and image using slicing\n",
    "# disparity_sub = disparity[::stride, ::stride]\n",
    "# left_img_sub = left_img[::stride, ::stride]\n",
    "\n",
    "# Recompute point cloud on the smaller disparity map\n",
    "# xyz_img_sub = compute_3D_from_disparity(P3, disparity_sub, R0_rect)\n",
    "\n",
    "# disp_points = xyz_img.reshape(-1, 3)\n",
    "# disp_points = disp_points[~np.isnan(disp_points).any(axis=1)]\n",
    "\n",
    "# Remove ground and sky of disparity point cloud\n",
    "# disp_points = remove_ground_plane(disp_points, height_threshold=1.3, axis='y', direction='below')\n",
    "# disp_points = remove_ground_plane(disp_points, height_threshold=-1.0, axis='y', direction='above')\n",
    "\n",
    "# Create colors as integers for valid points\n",
    "# valid_mask = ~np.isnan(xyz_img[:, :, 0])\n",
    "# img_colors = left_img[valid_mask]\n",
    "# img_colors = img_colors[:disp_points.shape[0]]\n",
    "# color_ints = bgr_to_int(img_colors)\n",
    "\n",
    "\n",
    "# Plot using K3D\n",
    "#     plot += k3d.points(positions=disp_points, colors=color_ints, point_size=0.025)\n",
    "# plot += k3d.points(positions=disp_points, point_size=0.025, colors=color_ints)\n",
    "\n",
    "\n",
    "\n",
    "labels_df = extract_labels(paths[\"labels\"])\n",
    "pred_lidar_df = extract_predictions(paths[\"pred_lidar\"])\n",
    "\n",
    "filtered_labels = filter_labels_in_fov(labels_df, R0_rect, fov_degrees=80)\n",
    "filtered_pred_lidar = filter_labels_in_fov(pred_lidar_df, R0_rect, fov_degrees=80)\n",
    "\n",
    "draw_3d_bounding_boxes_k3d_with_labels(plot, filtered_labels, R0_rect, color=0xff0000)\n",
    "draw_3d_bounding_boxes_k3d_with_labels(plot, filtered_pred_lidar, R0_rect, color=0x00ff00)\n",
    "\n",
    "\n",
    "print(filtered_pred_lidar)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot.camera = [0, 0, 0, 0, 0, 15, 0, -1.0, 0]\n",
    "plot.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
