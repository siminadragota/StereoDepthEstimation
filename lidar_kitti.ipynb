{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f8020b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import k3d\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6e7759df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- KITTI Utilities ---------- #\n",
    "\n",
    "# Load LIDAR point cloud from a .bin file\n",
    "def load_lidar_bin(file_path):\n",
    "    # Load binary data and reshape to N x 4 (x, y, z, reflectance)\n",
    "    points = np.fromfile(file_path, dtype=np.float32).reshape(-1, 4)\n",
    "    # Return only x, y, z (ignore reflectance)\n",
    "    return points[:, :3]\n",
    "\n",
    "# Extract object detection labels from KITTI label file\n",
    "def extract_labels(file_path):\n",
    "    columns = [\n",
    "        \"ObjectType\", \"Truncation\", \"Occlusion\", \"Alpha\", \n",
    "        \"X1\", \"Y1\", \"X2\", \"Y2\", \"H\", \"W\", \"L\", \"X\", \"Y\", \"Z\", \"Rotation_Y\"\n",
    "    ]\n",
    "    data = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            values = line.strip().split()\n",
    "            # Create a dictionary for each object, converting all values to float except ObjectType\n",
    "            object_data = {\n",
    "                columns[i]: values[i] if i == 0 else float(values[i])\n",
    "                for i in range(len(columns))\n",
    "            }\n",
    "            data.append(object_data)\n",
    "    # Return as a Pandas DataFrame\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load calibration matrices (projection, rectification, and transformation) from file\n",
    "def extract_matrices(filename):\n",
    "    global P2, P3, R0_rect, Tr_velo_to_cam, Tr_cam_to_velo\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.strip():\n",
    "                parts = line.split(':')\n",
    "                key = parts[0].strip()\n",
    "                values = np.fromstring(parts[1], sep=' ')\n",
    "                # Parse and reshape the appropriate matrix\n",
    "                if key == \"P2\":\n",
    "                    P2 = values.reshape(3, 4)\n",
    "                elif key == \"P3\":\n",
    "                    P3 = values.reshape(3, 4)\n",
    "                elif key == \"R0_rect\":\n",
    "                    R0_rect = values.reshape(3, 3)\n",
    "                elif key == \"Tr_velo_to_cam\":\n",
    "                    matrix_3x4 = values.reshape(3, 4)\n",
    "                    # Extend to 4x4 for homogeneous transformation\n",
    "                    Tr_velo_to_cam = np.vstack([matrix_3x4, np.array([[0, 0, 0, 1]])])\n",
    "                    # Inverse to get camera to lidar transformation\n",
    "                    Tr_cam_to_velo = np.linalg.inv(Tr_velo_to_cam)\n",
    "\n",
    "# # Compute 3D coordinates from disparity image using stereo projection matrix\n",
    "# def compute_3D_from_disparity(right_proj_mat, disparity_img, R0_rect):\n",
    "\n",
    "#     # Convert disparity from uint16 to float if needed\n",
    "#     if disparity_img.dtype == np.uint16:\n",
    "#         disparity_img = disparity_img.astype(np.float32) / 256.0\n",
    "\n",
    "#     height, width = disparity_img.shape\n",
    "#     xyz_img = np.zeros((height, width, 3), dtype=np.float32)\n",
    "\n",
    "#     # Extract camera intrinsics and stereo baseline offset\n",
    "#     f = right_proj_mat[0, 0]\n",
    "#     c_x = right_proj_mat[0, 2]\n",
    "#     c_y = right_proj_mat[1, 2]\n",
    "#     T_x = right_proj_mat[0, 3]\n",
    "\n",
    "#     print(\"Focal length: \", f)\n",
    "#     print(\"T_x: \", T_x)\n",
    "#     print(\"Baseline: \", -T_x/f)\n",
    "\n",
    "#     # Compute depth from disparity\n",
    "#     mask = disparity_img > 0\n",
    "#     depth = np.full_like(disparity_img, np.nan, dtype=np.float32)\n",
    "#     depth[mask] = (-T_x) / disparity_img[mask]\n",
    "\n",
    "#     # Generate pixel coordinate grid\n",
    "#     u, v = np.meshgrid(np.arange(width), np.arange(height))\n",
    "\n",
    "#     # Reconstruct 3D camera coordinates\n",
    "#     X = (u - c_x) * depth / f\n",
    "#     Y = (v - c_y) * depth / f\n",
    "#     Z = depth\n",
    "\n",
    "#     xyz_cam = np.stack((X, Y, Z), axis=-1)\n",
    "\n",
    "#     # Rectify using R0_rect\n",
    "#     xyz_rect = xyz_cam.reshape(-1, 3) @ R0_rect.T\n",
    "#     xyz_img = xyz_rect.reshape(height, width, 3)\n",
    "\n",
    "#     # Filter out distant points (e.g. sky or far background)\n",
    "#     xyz_img[xyz_img[:, :, 2] > 17.5] = np.nan\n",
    "#     return xyz_img\n",
    "\n",
    "def compute_3D_from_disparity(right_proj_mat, disparity_img, R0_rect=None):\n",
    "    \"\"\"\n",
    "    Compute a 3D point cloud from a disparity image using the stereo projection matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - right_proj_mat: 3x4 right projection matrix \n",
    "    - disparity_img: 2D array of disparity values\n",
    "    - R0_rect (optional): 3x3 rectification matrix to convert to rectified camera coordinates\n",
    "\n",
    "    Returns:\n",
    "    - xyz_img: (H, W, 3) array of 3D points in camera (or rectified) coordinates\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert disparity from uint16 to float if needed (KITTI format stores disparity * 256)\n",
    "    if disparity_img.dtype == np.uint16:\n",
    "        disparity_img = disparity_img.astype(np.float32) / 256.0\n",
    "\n",
    "    height, width = disparity_img.shape\n",
    "    xyz_img = np.full((height, width, 3), np.nan, dtype=np.float32)  # Default to NaNs\n",
    "\n",
    "    # Extract camera intrinsics\n",
    "    f = right_proj_mat[0, 0]             # Focal length (assumed fx = fy)\n",
    "    c_x = right_proj_mat[0, 2]           # Principal point x\n",
    "    c_y = right_proj_mat[1, 2]           # Principal point y\n",
    "    T_x = right_proj_mat[0, 3]           # Translation offset (fx * baseline)\n",
    "\n",
    "    # Compute baseline from T_x and focal length\n",
    "    baseline = -T_x / f            # baseline (in meters)\n",
    "\n",
    "    # Create a mask for valid disparity values\n",
    "    mask = disparity_img > 0\n",
    "    disparity = disparity_img.copy()\n",
    "\n",
    "    # Compute depth: Z = f * B / disparity\n",
    "    depth = np.full_like(disparity, np.nan, dtype=np.float32)\n",
    "    depth[mask] = f * baseline / disparity[mask]\n",
    "\n",
    "    # Generate mesh grid of pixel coordinates (u, v)\n",
    "    u, v = np.meshgrid(np.arange(width), np.arange(height))\n",
    "\n",
    "    # Compute X, Y, Z in camera coordinates\n",
    "    X = (u - c_x) * depth / f\n",
    "    Y = (v - c_y) * depth / f\n",
    "    Z = depth\n",
    "\n",
    "    # Stack into 3D points\n",
    "    xyz_cam = np.stack((X, Y, Z), axis=-1)\n",
    "\n",
    "    # Apply rectification if needed\n",
    "    if R0_rect is not None:\n",
    "        xyz_flat = xyz_cam.reshape(-1, 3)\n",
    "        xyz_rect = xyz_flat @ R0_rect.T\n",
    "        xyz_img = xyz_rect.reshape(height, width, 3)\n",
    "    else:\n",
    "        xyz_img = xyz_cam\n",
    "\n",
    "    # # Optional: mask out distant points (e.g., beyond 17.5 meters)\n",
    "    # xyz_img[xyz_img[:, :, 2] > 17.5] = np.nan\n",
    "    print(xyz_img.shape)\n",
    "\n",
    "    return xyz_img\n",
    "\n",
    "# def compute_3D_from_disparity(right_projection_matrix, disparity_img, R0_rect):\n",
    "#     \"\"\"\n",
    "#     Bereken de 3D-coördinaten (X, Y, Z) vanuit een disparity afbeelding en pas rectificatie toe.\n",
    "\n",
    "#     Parameters:\n",
    "#     - right_projection_matrix: 3x4 projectiematrix van de rechter camera.\n",
    "#     - disparity_img: Grayscale disparity afbeelding (numpy array of PNG-bestandspad).\n",
    "#     - R0_rect: 3x3 rectificatiematrix.\n",
    "\n",
    "#     Returns:\n",
    "#     - xyz_img: (hoogte x breedte x 3) array met XYZ-coördinaten of `np.nan` voor ongeldige pixels.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Controleer of disparity_img een bestandspad is en laad de afbeelding\n",
    "#     if isinstance(disparity_img, str):\n",
    "#         disparity_img = cv2.imread(disparity_img, cv2.IMREAD_UNCHANGED)  # Laad als 16-bits indien nodig\n",
    "#         if disparity_img is None:\n",
    "#             raise ValueError(f\"Kan de afbeelding niet laden: {disparity_img}\")\n",
    "    \n",
    "#     # Controleer of het een 16-bits disparity map is en schaal indien nodig\n",
    "#     if disparity_img.dtype == np.uint16:\n",
    "#         disparity_img = disparity_img.astype(np.float32) / 256.0  # Schalen als de disparity in 16-bits is opgeslagen\n",
    "    \n",
    "#     height, width = disparity_img.shape\n",
    "#     xyz_img = np.zeros((height, width, 3), dtype=np.float32)\n",
    "\n",
    "#     # Haal camera parameters uit de projectiematrix\n",
    "#     f = right_projection_matrix[0, 0]  # Focale lengte\n",
    "#     c_x = right_projection_matrix[0, 2]  # Optisch centrum X\n",
    "#     c_y = right_projection_matrix[1, 2]  # Optisch centrum Y\n",
    "#     T_x = right_projection_matrix[0, 3]  # Baseline (Tx is meestal negatief!)\n",
    "\n",
    "#     # Bereken de diepte Z\n",
    "#     no_occlusions = (disparity_img > 0)\n",
    "#     depth_img = np.full_like(disparity_img, np.nan, dtype=np.float32)\n",
    "#     depth_img[no_occlusions] = (-T_x) / disparity_img[no_occlusions]  # Diepte = baseline / disparity\n",
    "\n",
    "#     # Maak een raster van pixelcoördinaten\n",
    "#     u, v = np.meshgrid(np.arange(width), np.arange(height))\n",
    "\n",
    "#     # Camera frame (initiële 3D-coördinaten)\n",
    "#     X_cam = (u - c_x) * depth_img / f\n",
    "#     Y_cam = (v - c_y) * depth_img / f\n",
    "#     Z_cam = depth_img\n",
    "\n",
    "#     # Combineer XYZ-coördinaten in een Nx3 matrix\n",
    "#     xyz_flat = np.column_stack((X_cam.ravel(), Y_cam.ravel(), Z_cam.ravel()))  # (N, 3)\n",
    "\n",
    "#     # Pas rectificatie toe met R0_rect\n",
    "#     xyz_rect_flat = xyz_flat @ R0_rect.T  # Matrixvermenigvuldiging (3x3 rotatie)\n",
    "\n",
    "#     # Terugzetten naar (hoogte, breedte, 3) formaat\n",
    "#     xyz_img = xyz_rect_flat.reshape(height, width, 3)\n",
    "\n",
    "#     # Filter punten met Z > 20 meter\n",
    "#     mask = xyz_img[:, :, 2] > 17.5\n",
    "#     xyz_img[mask] = np.nan\n",
    "\n",
    "#     print(xyz_img.shape)\n",
    "    \n",
    "#     return xyz_img\n",
    "\n",
    "\n",
    "# Transform 3D LIDAR points from LIDAR to camera coordinate system\n",
    "def transform_lidar_to_camera(lidar_points, Tr_velo_to_cam):\n",
    "\n",
    "    # Convert to homogeneous coordinates by adding 1\n",
    "    ones = np.ones((lidar_points.shape[0], 1))\n",
    "    lidar_hom = np.hstack((lidar_points, ones))\n",
    "\n",
    "    # Apply transformation matrix\n",
    "    cam_points = lidar_hom @ Tr_velo_to_cam.T\n",
    "\n",
    "    return cam_points[:, :3]  # Return only x, y, z (discard homogeneous coordinate)\n",
    "\n",
    "# Display stereo image pair side by side\n",
    "def plot_img(left_path, right_path, figsize=(22, 6)):\n",
    "    img_left = mpimg.imread(left_path)\n",
    "    img_right = mpimg.imread(right_path)\n",
    "    combined_img = np.hstack((img_left, img_right))\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(combined_img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6b4951da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- KITTI Paths ---------- #\n",
    "\n",
    "# file_path_dataset = os.path.expanduser(\"~/computer_vision/Dataset\")\n",
    "# lidar_relative_path = \"/data_object_velodyne/training/velodyne/{:06d}.bin\"\n",
    "# left_img_relative_path = \"/data_object_image_2/training/image_2/{:06d}.png\"\n",
    "# right_img_relative_path = \"/data_object_image_3/training/image_3/{:06d}.png\"\n",
    "# disparity_relative_path = \"/disparity_images/disparity{:06d}.png\"\n",
    "# labels_relative_path = \"/data_object_label_2/training/label_2/{:06d}.txt\"\n",
    "# calib_relative_path = \"/data_object_calib/training/calib/{:06d}.txt\"\n",
    "\n",
    "file_path_dataset = os.path.expanduser(\"~/CV_Simina\")\n",
    "lidar_relative_path = \"/velodyne/{:06d}.bin\"\n",
    "left_img_relative_path = \"/image_2/{:06d}.png\"\n",
    "right_img_relative_path = \"/image_3/{:06d}.png\"\n",
    "disparity_relative_path = \"/disparity_images/disparity{:06d}.png\"\n",
    "labels_relative_path = \"/data_object_label_2/{:06d}.txt\"\n",
    "calib_relative_path =\"/data_object_calib/training/calib/{:06d}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5a0af088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(375, 1242, 3)\n"
     ]
    }
   ],
   "source": [
    "sequence_number = 4\n",
    "paths = {\"lidar\": file_path_dataset + lidar_relative_path.format(sequence_number),\n",
    "        \"left_img\": file_path_dataset + left_img_relative_path.format(sequence_number),\n",
    "        \"right_img\": file_path_dataset + right_img_relative_path.format(sequence_number),\n",
    "        \"disparity\": file_path_dataset + disparity_relative_path.format(sequence_number),\n",
    "        \"labels\": file_path_dataset + labels_relative_path.format(sequence_number),\n",
    "        \"calib\": file_path_dataset + calib_relative_path.format(sequence_number)}\n",
    "\n",
    "extract_matrices(paths[\"calib\"])\n",
    "lidar_points = load_lidar_bin(paths[\"lidar\"])\n",
    "lidar_points_cam = transform_lidar_to_camera(lidar_points, Tr_velo_to_cam)\n",
    "\n",
    "disparity = cv2.imread(paths[\"disparity\"], cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "xyz_img = compute_3D_from_disparity(P3, disparity, R0_rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dbc4628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Visualizer Function ---------- #\n",
    "\n",
    "def rgb_to_int(rgb_colors):\n",
    "    \"\"\"\n",
    "    rgb_colors: np.array of size Nx3\n",
    "    return: np.array of size N\n",
    "    \"\"\"\n",
    "    # Avoid overflow issues with uint8.\n",
    "    rgb_colors = rgb_colors.astype(np.uint32).T\n",
    "    int_colors = (rgb_colors[0] << 16) + (rgb_colors[1] << 8) + rgb_colors[2]\n",
    "    return int_colors\n",
    "\n",
    "def bgr_to_int(bgr_colors):\n",
    "    \"\"\"\n",
    "    bgr_colors: np.array of size Nx3\n",
    "    return: np.array of size N\n",
    "    \"\"\"\n",
    "    return rgb_to_int(bgr_colors[:, ::-1])\n",
    "\n",
    "def visualize_frame(sequence_number):\n",
    "    # Clear previous output \n",
    "    clear_output(wait=True)\n",
    "    print(f\"Frame: {sequence_number}\")\n",
    "\n",
    "    # Build absolute paths for the given frame\n",
    "    paths = {\n",
    "        \"lidar\": file_path_dataset + lidar_relative_path.format(sequence_number),\n",
    "        \"left_img\": file_path_dataset + left_img_relative_path.format(sequence_number),\n",
    "        \"right_img\": file_path_dataset + right_img_relative_path.format(sequence_number),\n",
    "        \"disparity\": file_path_dataset + disparity_relative_path.format(sequence_number),\n",
    "        \"labels\": file_path_dataset + labels_relative_path.format(sequence_number),\n",
    "        \"calib\": file_path_dataset + calib_relative_path.format(sequence_number),\n",
    "    }\n",
    "\n",
    "    # Load calibration matrices and transform LIDAR points to camera coordinates\n",
    "    extract_matrices(paths[\"calib\"])\n",
    "    lidar_points = load_lidar_bin(paths[\"lidar\"])\n",
    "    lidar_points_cam = transform_lidar_to_camera(lidar_points, Tr_velo_to_cam)\n",
    "\n",
    "    # Tr_cam3_to_cam2 = np.array([\n",
    "    # [1, 0, 0, 0],\n",
    "    # [0, 1, 0, 0.5],\n",
    "    # [0, 0, 1, 1],\n",
    "    # [0, 0, 0, 1]\n",
    "    # ])\n",
    "\n",
    "    # lidar_points_cam = transform_lidar_to_camera(lidar_points_cam, Tr_cam3_to_cam2)\n",
    "\n",
    "    # ---------- K3D Visualization ---------- #\n",
    "    plot = k3d.plot(camera_auto_fit=False, axes_helper=0.0)\n",
    "\n",
    "    # Plot LIDAR points in blue\n",
    "    lidar_points_cam_rect = (R0_rect @ lidar_points_cam.T).T\n",
    "    plot += k3d.points(lidar_points_cam_rect, point_size=0.05, color=0x0000ff)      # Plot non-rectified Lidar Pointcloud in blue\n",
    "    plot += k3d.points(lidar_points_cam, point_size=0.05, color=0x00ff00)           # Plot rectified LiDAR Pointcloud in green\n",
    "\n",
    "\n",
    "    # Load left image and disparity\n",
    "    left_img = cv2.imread(paths[\"left_img\"], cv2.IMREAD_COLOR)\n",
    "    disparity = cv2.imread(paths[\"disparity\"], cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "\n",
    "    xyz_img = compute_3D_from_disparity(P3, disparity)                             # Not rectified, add R0_rect to rectify it\n",
    "\n",
    "    # ## Downsample disparity maps --> only get the forth pixel for example\n",
    "    # # Subsample disparity map before computing 3D\n",
    "    # stride = 4  # every 4th pixel → 16x fewer points\n",
    "\n",
    "    # # Downsample disparity and image using slicing\n",
    "    # disparity_sub = disparity[::stride, ::stride]\n",
    "    # left_img_sub = left_img[::stride, ::stride]\n",
    "\n",
    "    # # Recompute point cloud on the smaller disparity map\n",
    "    # xyz_img_sub = compute_3D_from_disparity(P3, disparity_sub, R0_rect)\n",
    "\n",
    "    disp_points = xyz_img.reshape(-1, 3)\n",
    "    disp_points = disp_points[~np.isnan(disp_points).any(axis=1)]\n",
    "\n",
    "    # Create colors as integers for valid points\n",
    "    valid_mask = ~np.isnan(xyz_img[:, :, 0])\n",
    "    img_colors = left_img[valid_mask]\n",
    "    color_ints = bgr_to_int(img_colors)\n",
    "\n",
    "\n",
    "    # Plot using K3D\n",
    "    plot += k3d.points(positions=disp_points, colors=color_ints, point_size=0.025)\n",
    "    plot.camera = [0, 0, 0, 0, 0, 15, 0, -1.0, 0]\n",
    "\n",
    "    plot.display()\n",
    "\n",
    "\n",
    "    # # Visualize stereo-derived points with color based on depth (red to green)\n",
    "    # depth_vals = np.clip(disp_points[:, 2], 0, 17.5)\n",
    "    # color_vals = ((depth_vals / 17.5) * 255).astype(np.uint32)\n",
    "    # color_rgb = np.stack([255 - color_vals, color_vals, np.zeros_like(color_vals)], axis=1)\n",
    "\n",
    "    # # Convert RGB to packed RGBA uint32\n",
    "    # color_rgb_packed = (255 << 24) | (color_rgb[:, 0] << 16) | (color_rgb[:, 1] << 8) | color_rgb[:, 2]\n",
    "    # color_rgb_packed = color_rgb_packed.astype(np.uint32)\n",
    "\n",
    "    # # Plot with packed color array\n",
    "    # plot += k3d.points(positions=disp_points.astype(np.float32), colors=color_rgb_packed, point_size=0.05)\n",
    "\n",
    "\n",
    "    # # ---------- Point Cloud Alignment & Error Analysis ---------- #\n",
    "\n",
    "    # # Convert to Open3D point clouds\n",
    "    # pcd_disp = o3d.geometry.PointCloud()\n",
    "    # pcd_disp.points = o3d.utility.Vector3dVector(disp_points)\n",
    "    # pcd_lidar = o3d.geometry.PointCloud()\n",
    "    # pcd_lidar.points = o3d.utility.Vector3dVector(lidar_points_cam)\n",
    "\n",
    "    # # Remove statistical outliers to clean up noise\n",
    "    # pcd_disp, _ = pcd_disp.remove_statistical_outlier(20, 2.0)\n",
    "    # pcd_lidar, _ = pcd_lidar.remove_statistical_outlier(20, 2.0)\n",
    "\n",
    "    # # Compute Hausdorff distance BEFORE ICP alignment (measure of max discrepancy)\n",
    "    # d1 = pcd_disp.compute_point_cloud_distance(pcd_lidar)\n",
    "    # d2 = pcd_lidar.compute_point_cloud_distance(pcd_disp)\n",
    "    # hausdorff_before = max(max(d1), max(d2))\n",
    "    # print(f\"Hausdorff BEFORE ICP: {hausdorff_before:.3f} m\")\n",
    "\n",
    "    # # Align stereo-derived point cloud to LIDAR using ICP (Iterative Closest Point)\n",
    "    # result_icp = o3d.pipelines.registration.registration_icp(\n",
    "    #     pcd_disp, pcd_lidar, 0.5, np.eye(4),\n",
    "    #     o3d.pipelines.registration.TransformationEstimationPointToPoint()\n",
    "    # )\n",
    "\n",
    "    # # Show ICP result info\n",
    "    # print(\"ICP converged:\", result_icp.is_converged)\n",
    "    # print(\"ICP RMSE:\", result_icp.inlier_rmse)\n",
    "\n",
    "    # # Transform the disparity cloud using the estimated transformation\n",
    "    # pcd_disp.transform(result_icp.transformation)\n",
    "\n",
    "    # # Recompute Hausdorff distance AFTER ICP alignment\n",
    "    # d1 = pcd_disp.compute_point_cloud_distance(pcd_lidar)\n",
    "    # d2 = pcd_lidar.compute_point_cloud_distance(pcd_disp)\n",
    "    # hausdorff_after = max(max(d1), max(d2))\n",
    "    # print(f\"Hausdorff AFTER ICP: {hausdorff_after:.3f} m\")\n",
    "\n",
    "    # # Visualize aligned point clouds in Open3D viewer\n",
    "    # o3d.visualization.draw_geometries([pcd_disp, pcd_lidar])\n",
    "\n",
    "    # # Show stereo image pair\n",
    "    # plot_img(paths[\"left_img\"], paths[\"right_img\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6c68fa82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c260e4c4b0074aafac1b408f0ef0459f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Frame', max=9), Output()), _dom_classes=('widget-interac…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize_frame(sequence_number)>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- GUI Slider ---------- #\n",
    "\n",
    "slider = widgets.IntSlider(value=0, min=0, max=9, step=1, description=\"Frame\")\n",
    "widgets.interact(visualize_frame, sequence_number=slider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f75cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Export Functions ---------- #\n",
    "\n",
    "def save_pointcloud_bin(filename, points):\n",
    "    # Save N x 3 or N x 4 numpy array to .bin format\n",
    "    if points.shape[1] == 3:\n",
    "        points = np.hstack([points, np.zeros((points.shape[0], 1), dtype=np.float32)])  # add dummy intensity\n",
    "    points.astype(np.float32).tofile(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa19ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- Batch Processing ---------- #\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_all_frames(start=0, end=7481, export_dir=\"output/pointclouds\"):\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    hausdorff_log = []\n",
    "\n",
    "    for sequence_number in tqdm(range(start, end), desc=\"Processing KITTI frames\"):\n",
    "        try:\n",
    "            paths = {\n",
    "                \"lidar\": file_path_dataset + lidar_relative_path.format(sequence_number),\n",
    "                \"disparity\": file_path_dataset + disparity_relative_path.format(sequence_number),\n",
    "                \"calib\": file_path_dataset + calib_relative_path.format(sequence_number),\n",
    "            }\n",
    "\n",
    "            extract_matrices(paths[\"calib\"])\n",
    "            lidar_points = load_lidar_bin(paths[\"lidar\"])\n",
    "            lidar_cam = transform_lidar_to_camera(lidar_points, Tr_velo_to_cam)\n",
    "\n",
    "            disparity = cv2.imread(paths[\"disparity\"], cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "            xyz_img = compute_3D_from_disparity(P2, disparity, R0_rect)\n",
    "            disp_pc = xyz_img.reshape(-1, 3)\n",
    "            disp_pc = disp_pc[~np.isnan(disp_pc).any(axis=1)]\n",
    "\n",
    "            # Open3D ICP\n",
    "            pcd_disp = o3d.geometry.PointCloud()\n",
    "            pcd_disp.points = o3d.utility.Vector3dVector(disp_pc)\n",
    "            pcd_lidar = o3d.geometry.PointCloud()\n",
    "            pcd_lidar.points = o3d.utility.Vector3dVector(lidar_cam)\n",
    "            pcd_disp, _ = pcd_disp.remove_statistical_outlier(20, 2.0)\n",
    "            pcd_lidar, _ = pcd_lidar.remove_statistical_outlier(20, 2.0)\n",
    "\n",
    "            result_icp = o3d.pipelines.registration.registration_icp(\n",
    "                pcd_disp, pcd_lidar, 0.5, np.eye(4),\n",
    "                o3d.pipelines.registration.TransformationEstimationPointToPoint()\n",
    "            )\n",
    "            pcd_disp.transform(result_icp.transformation)\n",
    "            aligned_disp_pc = np.asarray(pcd_disp.points)\n",
    "\n",
    "            # Save aligned point clouds\n",
    "            save_pointcloud_bin(os.path.join(export_dir, f\"{sequence_number:06d}_lidar.bin\"), lidar_cam)\n",
    "            save_pointcloud_bin(os.path.join(export_dir, f\"{sequence_number:06d}_disp.bin\"), aligned_disp_pc)\n",
    "\n",
    "            # Log distances\n",
    "            d1 = pcd_disp.compute_point_cloud_distance(pcd_lidar)\n",
    "            d2 = pcd_lidar.compute_point_cloud_distance(pcd_disp)\n",
    "            hausdorff = max(max(d1), max(d2))\n",
    "            hausdorff_log.append((sequence_number, hausdorff))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error at frame {sequence_number}: {e}\")\n",
    "\n",
    "    # Save log\n",
    "    with open(os.path.join(export_dir, \"hausdorff_log.csv\"), \"w\") as f:\n",
    "        f.write(\"frame,hausdorff\")\n",
    "        for frame, dist in hausdorff_log:\n",
    "            f.write(f\"{frame},{dist:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42254aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- PointPillars Inference Placeholder ---------- #\n",
    "\n",
    "def run_pointpillars_inference(bin_file):\n",
    "    \"\"\"Placeholder for PointPillars inference.\"\"\"\n",
    "    # Simulate loading PointPillars model\n",
    "    print(f\"Running PointPillars on {bin_file} ...\")\n",
    "    # Example: simulate result with dummy box\n",
    "    dummy_boxes = [\n",
    "        [10, 0, 0, 1.6, 3.9, 1.5, 0]  # [x, y, z, w, l, h, yaw]\n",
    "    ]\n",
    "    return dummy_boxes\n",
    "\n",
    "def visualize_inference(bin_file):\n",
    "    points = np.fromfile(bin_file, dtype=np.float32).reshape(-1, 4)[:, :3]\n",
    "    bboxes = run_pointpillars_inference(bin_file)\n",
    "\n",
    "    plot = k3d.plot()\n",
    "    plot += k3d.points(points.astype(np.float32), point_size=0.05, color=0x888888)\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        x, y, z, w, l, h, heading = bbox\n",
    "        box = k3d.bounding_box(bounds=[x-w/2, y-l/2, z-h/2, x+w/2, y+l/2, z+h/2])\n",
    "        plot += box\n",
    "\n",
    "    plot.display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
