{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8020b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import k3d\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7759df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- KITTI Utilities ---------- #\n",
    "\n",
    "# Load LIDAR point cloud from a .bin file\n",
    "def load_lidar_bin(file_path):\n",
    "    # Load binary data and reshape to N x 4 (x, y, z, reflectance)\n",
    "    points = np.fromfile(file_path, dtype=np.float32).reshape(-1, 4)\n",
    "    # Return only x, y, z (ignore reflectance)\n",
    "    return points[:, :3]\n",
    "\n",
    "# Extract object detection labels from KITTI label file\n",
    "def extract_labels(file_path):\n",
    "    columns = [\n",
    "        \"ObjectType\", \"Truncation\", \"Occlusion\", \"Alpha\", \n",
    "        \"X1\", \"Y1\", \"X2\", \"Y2\", \"H\", \"W\", \"L\", \"X\", \"Y\", \"Z\", \"Rotation_Y\"\n",
    "    ]\n",
    "    data = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            values = line.strip().split()\n",
    "            # Create a dictionary for each object, converting all values to float except ObjectType\n",
    "            object_data = {\n",
    "                columns[i]: values[i] if i == 0 else float(values[i])\n",
    "                for i in range(len(columns))\n",
    "            }\n",
    "            data.append(object_data)\n",
    "    # Return as a Pandas DataFrame\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load calibration matrices (projection, rectification, and transformation) from file\n",
    "def extract_matrices(filename):\n",
    "    global P2, P3, R0_rect, Tr_velo_to_cam, Tr_cam_to_velo\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.strip():\n",
    "                parts = line.split(':')\n",
    "                key = parts[0].strip()\n",
    "                values = np.fromstring(parts[1], sep=' ')\n",
    "                # Parse and reshape the appropriate matrix\n",
    "                if key == \"P2\":\n",
    "                    P2 = values.reshape(3, 4)\n",
    "                elif key == \"P3\":\n",
    "                    P3 = values.reshape(3, 4)\n",
    "                elif key == \"R0_rect\":\n",
    "                    R0_rect = values.reshape(3, 3)\n",
    "                elif key == \"Tr_velo_to_cam\":\n",
    "                    matrix_3x4 = values.reshape(3, 4)\n",
    "                    # Extend to 4x4 for homogeneous transformation\n",
    "                    Tr_velo_to_cam = np.vstack([matrix_3x4, np.array([[0, 0, 0, 1]])])\n",
    "                    # Inverse to get camera to lidar transformation\n",
    "                    Tr_cam_to_velo = np.linalg.inv(Tr_velo_to_cam)\n",
    "\n",
    "def compute_3D_from_disparity(right_proj_mat, disparity_img, R0_rect=None):\n",
    "    \"\"\"\n",
    "    Compute a 3D point cloud from a disparity image using the stereo projection matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - right_proj_mat: 3x4 right projection matrix \n",
    "    - disparity_img: 2D array of disparity values\n",
    "    - R0_rect (optional): 3x3 rectification matrix to convert to rectified camera coordinates\n",
    "\n",
    "    Returns:\n",
    "    - xyz_img: (H, W, 3) array of 3D points in camera (or rectified) coordinates\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert disparity from uint16 to float if needed (KITTI format stores disparity * 256)\n",
    "    if disparity_img.dtype == np.uint16:\n",
    "        disparity_img = disparity_img.astype(np.float32) / 256.0\n",
    "\n",
    "    height, width = disparity_img.shape\n",
    "    xyz_img = np.full((height, width, 3), np.nan, dtype=np.float32)  # Default to NaNs\n",
    "\n",
    "    # Extract camera intrinsics\n",
    "    f = right_proj_mat[0, 0]             # Focal length (assumed fx = fy)\n",
    "    c_x = right_proj_mat[0, 2]           # Principal point x\n",
    "    c_y = right_proj_mat[1, 2]           # Principal point y\n",
    "    T_x = right_proj_mat[0, 3]           # Translation offset (fx * baseline)\n",
    "\n",
    "    # Compute baseline from T_x and focal length\n",
    "    baseline = -T_x / f            # baseline (in meters)\n",
    "\n",
    "    # Create a mask for valid disparity values\n",
    "    mask = disparity_img > 0\n",
    "    disparity = disparity_img.copy()\n",
    "\n",
    "    # Compute depth: Z = f * B / disparity\n",
    "    depth = np.full_like(disparity, np.nan, dtype=np.float32)\n",
    "    depth[mask] = f * baseline / disparity[mask]\n",
    "\n",
    "    # Generate mesh grid of pixel coordinates (u, v)\n",
    "    u, v = np.meshgrid(np.arange(width), np.arange(height))\n",
    "\n",
    "    # Compute X, Y, Z in camera coordinates\n",
    "    X = (u - c_x) * depth / f\n",
    "    Y = (v - c_y) * depth / f\n",
    "    Z = depth\n",
    "\n",
    "    # Stack into 3D points\n",
    "    xyz_cam = np.stack((X, Y, Z), axis=-1)\n",
    "\n",
    "    # Apply rectification if needed\n",
    "    if R0_rect is not None:\n",
    "        xyz_flat = xyz_cam.reshape(-1, 3)\n",
    "        xyz_rect = R0_rect @ xyz_flat.T\n",
    "        xyz_img = xyz_rect.reshape(height, width, 3)\n",
    "    else:\n",
    "        xyz_img = xyz_cam\n",
    "\n",
    "    # # Optional: mask out distant points (e.g., beyond 17.5 meters)\n",
    "    # xyz_img[xyz_img[:, :, 2] > 17.5] = np.nan\n",
    "    print(xyz_img.shape)\n",
    "\n",
    "    return xyz_img\n",
    "\n",
    "\n",
    "def filter_lidar_fov(points, fov_degrees=120):\n",
    "    \"\"\"\n",
    "    Filters 3D LIDAR points to keep only those within a given horizontal field of view (in front of the camera).\n",
    "    \n",
    "    Parameters:\n",
    "    - points: Nx3 array of points in camera coordinates\n",
    "    - fov_degrees: field of view angle in degrees (default: 120)\n",
    "\n",
    "    Returns:\n",
    "    - Filtered points within the field of view\n",
    "    \"\"\"\n",
    "    # Compute horizontal angles (in degrees) relative to camera's forward axis\n",
    "    x, y = points[:, 0], points[:, 1]\n",
    "    angles = np.degrees(np.arctan2(y, x))  # Angle in degrees between -180 and 180\n",
    "\n",
    "    # Define limits (centered around 0)\n",
    "    half_fov = fov_degrees / 2.0\n",
    "    mask = (angles >= -half_fov) & (angles <= half_fov)\n",
    "\n",
    "    return points[mask]\n",
    "\n",
    "def compute_hausdorff_and_stats(pcd1, pcd2):\n",
    "    \"\"\"Compute symmetric Hausdorff and average distances.\"\"\"\n",
    "    d1 = np.asarray(pcd1.compute_point_cloud_distance(pcd2))\n",
    "    d2 = np.asarray(pcd2.compute_point_cloud_distance(pcd1))\n",
    "\n",
    "    hausdorff = max(np.max(d1), np.max(d2))\n",
    "    mean1, mean2 = np.mean(d1), np.mean(d2)\n",
    "    percentile_95 = np.percentile(np.concatenate([d1, d2]), 95)\n",
    "\n",
    "    return hausdorff, mean1, mean2, percentile_95\n",
    "\n",
    "# Transform 3D LIDAR points from LIDAR to camera coordinate system\n",
    "def transform_lidar_to_camera(lidar_points, Tr_velo_to_cam):\n",
    "\n",
    "    # Convert to homogeneous coordinates by adding 1\n",
    "    ones = np.ones((lidar_points.shape[0], 1))\n",
    "    lidar_hom = np.hstack((lidar_points, ones))\n",
    "\n",
    "    # Apply transformation matrix\n",
    "    cam_points = lidar_hom @ Tr_velo_to_cam.T\n",
    "\n",
    "    return cam_points[:, :3]  # Return only x, y, z (discard homogeneous coordinate)\n",
    "\n",
    "# Display stereo image pair side by side\n",
    "def plot_img(left_path, right_path, figsize=(22, 6)):\n",
    "    img_left = mpimg.imread(left_path)\n",
    "    img_right = mpimg.imread(right_path)\n",
    "    combined_img = np.hstack((img_left, img_right))\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(combined_img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4951da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- KITTI Paths ---------- #\n",
    "\n",
    "# file_path_dataset = os.path.expanduser(\"~/computer_vision/Dataset\")\n",
    "# lidar_relative_path = \"/data_object_velodyne/training/velodyne/{:06d}.bin\"\n",
    "# left_img_relative_path = \"/data_object_image_2/training/image_2/{:06d}.png\"\n",
    "# right_img_relative_path = \"/data_object_image_3/training/image_3/{:06d}.png\"\n",
    "# disparity_relative_path = \"/disparity_images/disparity{:06d}.png\"\n",
    "# labels_relative_path = \"/data_object_label_2/training/label_2/{:06d}.txt\"\n",
    "# calib_relative_path = \"/data_object_calib/training/calib/{:06d}.txt\"\n",
    "\n",
    "file_path_dataset = os.path.expanduser(\"~/CV_Simina\")\n",
    "lidar_relative_path = \"/velodyne/{:06d}.bin\"\n",
    "left_img_relative_path = \"/image_2/{:06d}.png\"\n",
    "right_img_relative_path = \"/image_3/{:06d}.png\"\n",
    "disparity_relative_path = \"/disparity_images/disparity{:06d}.png\"\n",
    "labels_relative_path = \"/data_object_label_2/{:06d}.txt\"\n",
    "calib_relative_path =\"/data_object_calib/training/calib/{:06d}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0af088",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_number = 4\n",
    "paths = {\"lidar\": file_path_dataset + lidar_relative_path.format(sequence_number),\n",
    "        \"left_img\": file_path_dataset + left_img_relative_path.format(sequence_number),\n",
    "        \"right_img\": file_path_dataset + right_img_relative_path.format(sequence_number),\n",
    "        \"disparity\": file_path_dataset + disparity_relative_path.format(sequence_number),\n",
    "        \"labels\": file_path_dataset + labels_relative_path.format(sequence_number),\n",
    "        \"calib\": file_path_dataset + calib_relative_path.format(sequence_number)}\n",
    "\n",
    "extract_matrices(paths[\"calib\"])\n",
    "lidar_points = load_lidar_bin(paths[\"lidar\"])\n",
    "lidar_points_cam = transform_lidar_to_camera(lidar_points, Tr_velo_to_cam)\n",
    "lidar_points_cam = filter_lidar_fov(lidar_points_cam, fov_degrees=120)\n",
    "\n",
    "disparity = cv2.imread(paths[\"disparity\"], cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "xyz_img = compute_3D_from_disparity(P3, disparity, R0_rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc4628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Visualizer Function ---------- #\n",
    "\n",
    "def rgb_to_int(rgb_colors):\n",
    "    \"\"\"\n",
    "    rgb_colors: np.array of size Nx3\n",
    "    return: np.array of size N\n",
    "    \"\"\"\n",
    "    # Avoid overflow issues with uint8.\n",
    "    rgb_colors = rgb_colors.astype(np.uint32).T\n",
    "    int_colors = (rgb_colors[0] << 16) + (rgb_colors[1] << 8) + rgb_colors[2]\n",
    "    return int_colors\n",
    "\n",
    "def bgr_to_int(bgr_colors):\n",
    "    \"\"\"\n",
    "    bgr_colors: np.array of size Nx3\n",
    "    return: np.array of size N\n",
    "    \"\"\"\n",
    "    return rgb_to_int(bgr_colors[:, ::-1])\n",
    "\n",
    "def visualize_frame(sequence_number):\n",
    "    # Clear previous output \n",
    "    clear_output(wait=True)\n",
    "    print(f\"Frame: {sequence_number}\")\n",
    "\n",
    "    # Build absolute paths for the given frame\n",
    "    paths = {\n",
    "        \"lidar\": file_path_dataset + lidar_relative_path.format(sequence_number),\n",
    "        \"left_img\": file_path_dataset + left_img_relative_path.format(sequence_number),\n",
    "        \"right_img\": file_path_dataset + right_img_relative_path.format(sequence_number),\n",
    "        \"disparity\": file_path_dataset + disparity_relative_path.format(sequence_number),\n",
    "        \"labels\": file_path_dataset + labels_relative_path.format(sequence_number),\n",
    "        \"calib\": file_path_dataset + calib_relative_path.format(sequence_number),\n",
    "    }\n",
    "\n",
    "    # Load calibration matrices and transform LIDAR points to camera coordinates\n",
    "    extract_matrices(paths[\"calib\"])\n",
    "    lidar_points = load_lidar_bin(paths[\"lidar\"])\n",
    "    lidar_points_cam = filter_lidar_fov(lidar_points, fov_degrees=90)\n",
    "    lidar_points_cam = transform_lidar_to_camera(lidar_points_cam, Tr_velo_to_cam)\n",
    "    \n",
    "    # Tr_cam3_to_cam2 = np.array([\n",
    "    # [1, 0, 0, 0],\n",
    "    # [0, 1, 0, 0.5],\n",
    "    # [0, 0, 1, 1],\n",
    "    # [0, 0, 0, 1]\n",
    "    # ])\n",
    "\n",
    "    # lidar_points_cam = transform_lidar_to_camera(lidar_points_cam, Tr_cam3_to_cam2)\n",
    "\n",
    "    # ---------- K3D Visualization ---------- #\n",
    "    plot = k3d.plot(camera_auto_fit=False, axes_helper=0.0)\n",
    "\n",
    "    # Plot LIDAR points in blue\n",
    "    lidar_points_cam_rect = (R0_rect @ lidar_points_cam.T).T\n",
    "    plot += k3d.points(lidar_points_cam_rect, point_size=0.05, color=0x0000ff)      # Plot non-rectified Lidar Pointcloud in blue\n",
    "    plot += k3d.points(lidar_points_cam, point_size=0.05, color=0x00ff00)           # Plot rectified LiDAR Pointcloud in green\n",
    "\n",
    "\n",
    "    # Load left image and disparity\n",
    "    left_img = cv2.imread(paths[\"left_img\"], cv2.IMREAD_COLOR)\n",
    "    disparity = cv2.imread(paths[\"disparity\"], cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "\n",
    "    xyz_img = compute_3D_from_disparity(P3, disparity)                             # Not rectified, add R0_rect to rectify it\n",
    "\n",
    "    ## Downsample disparity maps --> only get the forth pixel for example\n",
    "    # Subsample disparity map before computing 3D\n",
    "    stride = 4  # every 4th pixel → 16x fewer points\n",
    "\n",
    "    # Downsample disparity and image using slicing\n",
    "    disparity_sub = disparity[::stride, ::stride]\n",
    "    left_img_sub = left_img[::stride, ::stride]\n",
    "\n",
    "    # Recompute point cloud on the smaller disparity map\n",
    "    xyz_img_sub = compute_3D_from_disparity(P3, disparity_sub, R0_rect)\n",
    "\n",
    "    disp_points = xyz_img.reshape(-1, 3)\n",
    "    disp_points = disp_points[~np.isnan(disp_points).any(axis=1)]\n",
    "\n",
    "    # Create colors as integers for valid points\n",
    "    valid_mask = ~np.isnan(xyz_img[:, :, 0])\n",
    "    img_colors = left_img[valid_mask]\n",
    "    color_ints = bgr_to_int(img_colors)\n",
    "\n",
    "\n",
    "    # Plot using K3D\n",
    "    plot += k3d.points(positions=disp_points, colors=color_ints, point_size=0.025)\n",
    "    plot.camera = [0, 0, 0, 0, 0, 15, 0, -1.0, 0]\n",
    "\n",
    "    plot.display()\n",
    "\n",
    "\n",
    "    # # Visualize stereo-derived points with color based on depth (red to green)\n",
    "    # depth_vals = np.clip(disp_points[:, 2], 0, 17.5)\n",
    "    # color_vals = ((depth_vals / 17.5) * 255).astype(np.uint32)\n",
    "    # color_rgb = np.stack([255 - color_vals, color_vals, np.zeros_like(color_vals)], axis=1)\n",
    "\n",
    "    # # Convert RGB to packed RGBA uint32\n",
    "    # color_rgb_packed = (255 << 24) | (color_rgb[:, 0] << 16) | (color_rgb[:, 1] << 8) | color_rgb[:, 2]\n",
    "    # color_rgb_packed = color_rgb_packed.astype(np.uint32)\n",
    "\n",
    "    # # Plot with packed color array\n",
    "    # plot += k3d.points(positions=disp_points.astype(np.float32), colors=color_rgb_packed, point_size=0.05)\n",
    "\n",
    "\n",
    "    # # ---------- Point Cloud Alignment & Error Analysis ---------- #\n",
    "\n",
    "    # Convert to Open3D point clouds\n",
    "    pcd_disp = o3d.geometry.PointCloud()\n",
    "    pcd_disp.points = o3d.utility.Vector3dVector(disp_points)\n",
    "    pcd_lidar = o3d.geometry.PointCloud()\n",
    "    pcd_lidar.points = o3d.utility.Vector3dVector(lidar_points_cam)\n",
    "\n",
    "    # Remove statistical outliers to clean up noise\n",
    "    pcd_disp, _ = pcd_disp.remove_statistical_outlier(20, 2.0)\n",
    "    pcd_lidar, _ = pcd_lidar.remove_statistical_outlier(20, 2.0)\n",
    "\n",
    "    # BEFORE ICP\n",
    "    hd_before, mean1_b, mean2_b, p95_b = compute_hausdorff_and_stats(pcd_disp, pcd_lidar)\n",
    "    print(f\"Hausdorff BEFORE ICP: {hd_before:.3f} m | Mean d1→2: {mean1_b:.3f}, d2→1: {mean2_b:.3f} | 95th%: {p95_b:.3f}\")\n",
    "\n",
    "    # Run ICP\n",
    "    result_icp = o3d.pipelines.registration.registration_icp(\n",
    "        pcd_disp, pcd_lidar, max_correspondence_distance=0.5, init=np.eye(4),\n",
    "        estimation_method=o3d.pipelines.registration.TransformationEstimationPointToPoint()\n",
    "    )\n",
    "\n",
    "    # Print ICP results\n",
    "    print(f\"ICP Fitness: {result_icp.fitness:.4f}\")\n",
    "    print(f\"ICP RMSE: {result_icp.inlier_rmse:.4f}\")\n",
    "\n",
    "    # Apply transformation\n",
    "    pcd_disp.transform(result_icp.transformation)\n",
    "\n",
    "    # AFTER ICP\n",
    "    hd_after, mean1_a, mean2_a, p95_a = compute_hausdorff_and_stats(pcd_disp, pcd_lidar)\n",
    "    print(f\"Hausdorff AFTER ICP: {hd_after:.3f} m | Mean d1→2: {mean1_a:.3f}, d2→1: {mean2_a:.3f} | 95th%: {p95_a:.3f}\")\n",
    "\n",
    "\n",
    "    # Visualize aligned point clouds in Open3D viewer\n",
    "    o3d.visualization.draw_geometries([pcd_disp, pcd_lidar])\n",
    "\n",
    "    # Show stereo image pair\n",
    "    plot_img(paths[\"left_img\"], paths[\"right_img\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68fa82",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ---------- GUI Slider ---------- #\n",
    "\n",
    "slider = widgets.IntSlider(value=4, min=4, max=4, step=1, description=\"Frame\")\n",
    "widgets.interact(visualize_frame, sequence_number=slider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "578f75cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Export Functions ---------- #\n",
    "\n",
    "def save_pointcloud_bin(filename, points):\n",
    "    # Save N x 3 or N x 4 numpy array to .bin format\n",
    "    if points.shape[1] == 3:\n",
    "        points = np.hstack([points, np.zeros((points.shape[0], 1), dtype=np.float32)])  # add dummy intensity\n",
    "    points.astype(np.float32).tofile(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa19ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- Batch Processing ---------- #\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_all_frames(start=0, end=7481, export_dir=\"output/pointclouds\"):\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    hausdorff_log = []\n",
    "\n",
    "    for sequence_number in tqdm(range(start, end), desc=\"Processing KITTI frames\"):\n",
    "        try:\n",
    "            paths = {\n",
    "                \"lidar\": file_path_dataset + lidar_relative_path.format(sequence_number),\n",
    "                \"disparity\": file_path_dataset + disparity_relative_path.format(sequence_number),\n",
    "                \"calib\": file_path_dataset + calib_relative_path.format(sequence_number),\n",
    "            }\n",
    "\n",
    "            extract_matrices(paths[\"calib\"])\n",
    "            lidar_points = load_lidar_bin(paths[\"lidar\"])\n",
    "            lidar_cam = transform_lidar_to_camera(lidar_points, Tr_velo_to_cam)\n",
    "\n",
    "            disparity = cv2.imread(paths[\"disparity\"], cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "            xyz_img = compute_3D_from_disparity(P2, disparity, R0_rect)\n",
    "            disp_pc = xyz_img.reshape(-1, 3)\n",
    "            disp_pc = disp_pc[~np.isnan(disp_pc).any(axis=1)]\n",
    "\n",
    "            # Open3D ICP\n",
    "            pcd_disp = o3d.geometry.PointCloud()\n",
    "            pcd_disp.points = o3d.utility.Vector3dVector(disp_pc)\n",
    "            pcd_lidar = o3d.geometry.PointCloud()\n",
    "            pcd_lidar.points = o3d.utility.Vector3dVector(lidar_cam)\n",
    "            pcd_disp, _ = pcd_disp.remove_statistical_outlier(20, 2.0)\n",
    "            pcd_lidar, _ = pcd_lidar.remove_statistical_outlier(20, 2.0)\n",
    "\n",
    "            result_icp = o3d.pipelines.registration.registration_icp(\n",
    "                pcd_disp, pcd_lidar, 0.5, np.eye(4),\n",
    "                o3d.pipelines.registration.TransformationEstimationPointToPoint()\n",
    "            )\n",
    "            pcd_disp.transform(result_icp.transformation)\n",
    "            aligned_disp_pc = np.asarray(pcd_disp.points)\n",
    "\n",
    "            # Save aligned point clouds\n",
    "            save_pointcloud_bin(os.path.join(export_dir, f\"{sequence_number:06d}_lidar.bin\"), lidar_cam)\n",
    "            save_pointcloud_bin(os.path.join(export_dir, f\"{sequence_number:06d}_disp.bin\"), aligned_disp_pc)\n",
    "\n",
    "            # Log distances\n",
    "            d1 = pcd_disp.compute_point_cloud_distance(pcd_lidar)\n",
    "            d2 = pcd_lidar.compute_point_cloud_distance(pcd_disp)\n",
    "            hausdorff = max(max(d1), max(d2))\n",
    "            hausdorff_log.append((sequence_number, hausdorff))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error at frame {sequence_number}: {e}\")\n",
    "\n",
    "    # Save log\n",
    "    with open(os.path.join(export_dir, \"hausdorff_log.csv\"), \"w\") as f:\n",
    "        f.write(\"frame,hausdorff\")\n",
    "        for frame, dist in hausdorff_log:\n",
    "            f.write(f\"{frame},{dist:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b42254aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- PointPillars Inference Placeholder ---------- #\n",
    "\n",
    "def run_pointpillars_inference(bin_file):\n",
    "    \"\"\"Placeholder for PointPillars inference.\"\"\"\n",
    "    # Simulate loading PointPillars model\n",
    "    print(f\"Running PointPillars on {bin_file} ...\")\n",
    "    # Example: simulate result with dummy box\n",
    "    dummy_boxes = [\n",
    "        [10, 0, 0, 1.6, 3.9, 1.5, 0]  # [x, y, z, w, l, h, yaw]\n",
    "    ]\n",
    "    return dummy_boxes\n",
    "\n",
    "def visualize_inference(bin_file):\n",
    "    points = np.fromfile(bin_file, dtype=np.float32).reshape(-1, 4)[:, :3]\n",
    "    bboxes = run_pointpillars_inference(bin_file)\n",
    "\n",
    "    plot = k3d.plot()\n",
    "    plot += k3d.points(points.astype(np.float32), point_size=0.05, color=0x888888)\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        x, y, z, w, l, h, heading = bbox\n",
    "        box = k3d.bounding_box(bounds=[x-w/2, y-l/2, z-h/2, x+w/2, y+l/2, z+h/2])\n",
    "        plot += box\n",
    "\n",
    "    plot.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630aeed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
